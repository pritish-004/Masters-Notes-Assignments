{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Facebook_dataset_training.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "blGeVSxaTSbv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "outputId": "4d8756fa-c24e-41ed-c954-0a44bc043d79"
   },
   "source": [
    "pip install pattern3"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern3 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
      "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.6/dist-packages (from pattern3) (20200517)\n",
      "Requirement already satisfied: cherrypy in /usr/local/lib/python3.6/dist-packages (from pattern3) (18.6.0)\n",
      "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (from pattern3) (5.2.1)\n",
      "Requirement already satisfied: docx in /usr/local/lib/python3.6/dist-packages (from pattern3) (0.2.4)\n",
      "Requirement already satisfied: pdfminer3k in /usr/local/lib/python3.6/dist-packages (from pattern3) (1.3.4)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.6/dist-packages (from pattern3) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from pattern3) (4.6.3)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.6/dist-packages (from pdfminer.six->pattern3) (3.9.7)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six->pattern3) (3.0.4)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six->pattern3) (2.2.2)\n",
      "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from cherrypy->pattern3) (2.6)\n",
      "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.6/dist-packages (from cherrypy->pattern3) (2.0)\n",
      "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.6/dist-packages (from cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from cherrypy->pattern3) (8.4.0)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.6/dist-packages (from cherrypy->pattern3) (8.3.0)\n",
      "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.6/dist-packages (from docx->pattern3) (7.0.0)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from docx->pattern3) (4.2.6)\n",
      "Requirement already satisfied: ply in /usr/local/lib/python3.6/dist-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.6/dist-packages (from portend>=2.1.1->cherrypy->pattern3) (3.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zc.lockfile->cherrypy->pattern3) (47.3.1)\n",
      "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from jaraco.collections->cherrypy->pattern3) (1.12.0)\n",
      "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.6/dist-packages (from jaraco.collections->cherrypy->pattern3) (3.2.0)\n",
      "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.6/dist-packages (from jaraco.collections->cherrypy->pattern3) (3.1.0)\n",
      "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.6/dist-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.9)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.7\"->jaraco.text->jaraco.collections->cherrypy->pattern3) (1.6.1)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.7\"->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.1.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0JQqglOlSd9N",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "#For training this classifier we need pattern, nltk (including the corpus), re and csv modules#\n",
    "\n",
    "from pattern3.vector import Model, Document, BINARY, SVM, kfoldcv, IG, SLP,KNN, NB\n",
    "from pattern3.db import csv\n",
    "#from pattern3.en import ngrams\n",
    "from pattern3.vector import stem, PORTER, LEMMA\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv as csv1\n"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Jn4JBzDTlzE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "data = csv('Facebook_dataset.csv')\n",
    "data = [[message, int(side_effect_indicator)] for message, side_effect_indicator in data]\n",
    "\n",
    "#List of nltk stopwords\n",
    "stop = [u'i', 'm', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself',\n",
    "             u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its',\n",
    "             u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom',\n",
    "             u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being',\n",
    "             u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but',\n",
    "             u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against',\n",
    "             u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up',\n",
    "             u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when',\n",
    "             u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor',\n",
    "             u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don',\n",
    "             u'should', u'now']\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OeM21MtWTr0p",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Adding medicine names and obvious names into the stop words\n",
    "#\n",
    "# The following list was extacted essentially from wikipedia, webmd and mayoclinic websites. \n",
    "# These are all related to Glucophage and they conceptually mean the same thing and hence it should be removed\n",
    "# We want data only related to adverse drug reactions  \n",
    "# Pioglitazone is marketed as trademarks Actos in the USA, Canada, the UK and Germany, Glustin in Europe, \n",
    "# Glizone and Pioz in India by Zydus Cadila and USV respectively and Zactos in Mexico by Takeda Pharmaceuticals.\n",
    "similar_mean_list1 = [\"diabetes\",\"actos\", \"pioglitazone hydrochloride\", \"pioglitazone\",  \"glustin\", \"glizone\", \"pioz\", \"zactos\"]\n",
    "#The name \"Metformin\" is the BAN(British Approved Name), USAN(United States Approved Name ) and INN(International Nonproprietary Names) \n",
    "#for the medication. It is sold under several trade names, including Glucophage XR, Carbophage SR, Riomet, \n",
    "#Fortamet, Glumetza, Obimet, Gluformin, Dianben, Diabex, Diaformin, Siofor, Metfogamma and Glifor.\n",
    "similar_mean_list2 = [\"medformin\",\"glucophage\", \"metformin\", \"glucophage xr\", \"metformin hydrochloride\", \"carbophage sr\", \"riomet\", \"fortamet\", \"glumetza\", \"obimet\", \"gluformin\", \"dianben\", \"diabex\", \"diaformin\", \"siofor\",\"metfogamma\", \"riomet\"]\n",
    "#Byetta and Bydureon are injectable medications used to treat type 2 diabetes.\n",
    "similar_mean_list3 = [\"byetta\", \"bydureon\", \"exenatide\"]\n",
    "#Liraglutide is marketed under the brand name Victoza in U.K. UAE, Kuwait, India and Saxenda in Australia,\n",
    "#Iran, Israel, Canada, Brazil, and the U.S\n",
    "similar_mean_list4 = [\"victoza\", \"liraglutide\", \"saxenda\"]\n",
    "#Canagliflozin, sold under the brand name Invokana among others, \n",
    "#is a medication used to treat type 2 diabetes.\n",
    "similar_mean_list5 = [\"invokana\", \"canagliflozin\"]\n",
    "#Rosiglitazone (trade name Avandia) is an antidiabetic drug in the thiazolidinedione class. \n",
    "similar_mean_list6 = [\"avandia\", \"rosiglitazone\"]\n",
    "# these are the types of insulin which mean the same thing\n",
    "similar_mean_list7 = [\"insulin glargine\",  \"lantus\", \"toujeo\", \"abasaglar\", \"basaglar\",\"insulin\"]\n",
    "#Januvia (sitagliptin) and (sitagliptin/metformin) are oral diabetes medicines\n",
    "similar_mean_list8 = [\"sitagliptin\", \"janumet\", \"januvia\", \"juvisync\"]\n",
    "#Glimepiride, sold under the trade name Amaryl among others, is a medication used to treat diabetes mellitus type 2.\n",
    "similar_mean_list9 = [\"amaryl\", \"glimepiride\", \"gleam\", \"k-glim-1\", \"glucoryl\",  \"glimpid\", \"glimy\"]\n",
    "# remove words related to diabetes\n",
    "similar_mean_list = similar_mean_list1 + similar_mean_list2 + similar_mean_list3 + similar_mean_list4 + similar_mean_list5 + similar_mean_list6 + similar_mean_list7 + similar_mean_list8 + similar_mean_list9\n",
    "# Removing these stop words and general cleaning\n",
    "newsimilar_mean_list = []\n",
    "\n",
    "#converting to unicode#\n",
    "for r in similar_mean_list:\n",
    "    newsimilar_mean_list.append(r.encode(\"utf-8\"))\n",
    "    \n",
    "#now stop words include medicine names and common words in English#\n",
    "stop = stop + newsimilar_mean_list\n",
    "\n",
    "#This will be our dataset, extracted the non-null and non-empty comments \n",
    "refineddata = [[c[0].lower(),c[1]] for c in data if c[0] != \"\" and c[0] != None]\n"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wMC3uBo1Twrf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Function to convert the text into features(single words, bigrams and trigrams)#\n",
    "def features(message):\n",
    "    # Removing these stop words and general cleaning\n",
    "    singlegrams =  [i for i in message.split() if i not in stop] \n",
    "    singlegramsrefined = []\n",
    "    #Stemming the single words\n",
    "    #The stemming takes to unusual roots, so lemmatization is applied\n",
    "    for k in singlegrams:\n",
    "        r = stem(k, stemmer=LEMMA)\n",
    "        singlegramsrefined.append(r)\n",
    "    newmessage = \" \".join(singlegramsrefined)\n",
    "    # Removing numbers\n",
    "    newmessage = re.sub(\"[^A-Za-z]\", \" \", newmessage)\n",
    "    # Removing stopwords\n",
    "    newmessage = re.sub(r'[^\\w]', ' ', newmessage)\n",
    "    #Again splitting to single grams\n",
    "    singlegrams= [i for i in newmessage.split()]\n",
    "    singlegramsrefined2 = []\n",
    "\n",
    "    for word in singlegrams:\n",
    "        singlegramsrefined2.append(word)\n",
    "    \n",
    "    totalgrams = singlegramsrefined2     \n",
    "    return totalgrams"
   ],
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pppfETF_T4oY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "outputId": "54da167b-6c9f-47da-d8f9-951d68192d7a"
   },
   "source": [
    "#The cleaned input dataset is passed to features function \n",
    "refineddata1 = [(features(c[0]),c[1]) for c in refineddata]\n",
    "#print(refineddata1)\n",
    "#Each datapoint becomes a pattern document here; The type represents the label for each document#\n",
    "#A Document is an unordered bag-of-words representation of a given string, dictionary of (word, count)-items, Sentence or Text.\n",
    "refineddata2 = [Document(message, type=sideeffectindicator) for message, sideeffectindicator in refineddata1]\n",
    "#print(refineddata2)\n",
    "# Defining the model using the documents; feature weight is Information gain; \n",
    "# The weight parameter can be set to TF (relative term frequency), \n",
    "# TFIDF, (term frequency vs. document frequency), IG (information gain), BINARY (0 or 1) or None (original weights).\n",
    "# Bag-of-words means that the word order in the given text is discarded. \n",
    "# Instead, words are counted using the words(), count() and stem() and functions.\n",
    "model = Model(documents=refineddata2, weight=IG)\n",
    "\n",
    "#Top 50 features selected#\n",
    "features=model.feature_selection(top=50)\n",
    "\n",
    "#If medicine names are present in the features, they are removed #\n",
    "refinedfeatures = []\n",
    "for i in features:\n",
    "    if i not in similar_mean_list:\n",
    "        refinedfeatures.append(i)\n",
    "\n",
    "#After few rounds of examining the top features the following top features will be removed to make the model more general#\n",
    "        \n",
    "skipList1 = [u'mg',u'release',u'slow', u'release',u'take', u'metformin',u'mg', u'twice',u'extended', u'release']\n",
    "\n",
    "skipList2 = [u'glipizide',u'metformin', u'i', u'day', u'twice', u'started',u'switched',u'months',u'stopped']\n",
    "\n",
    "skipList3 = [u'made',u'med', u'insulin',u'morning']\n",
    "\n",
    "skipList4 = [u'taking',u'gliclazide',u'acid', u'changing']\n",
    "\n",
    "skipList5 = [u'form',u'first', u'started',u'fog', u'storm',u'storm', u'low']\n",
    "\n",
    "skipList6 = [ u'tested',u'like',u'storm',u'lipoic', u'acid', u'mg']\n",
    "\n",
    "skipList7 = [ u'horrible',u'metformin', u'made', u'sick',u'problem',u'one']\n",
    "\n",
    "skipList8 = [u'told',u'took']\n",
    "\n",
    "skipList9 = [u'x',u'tablet',u'glyberide',u'jentadueto',u'snd', u'time',u'time']\n",
    "\n",
    "skipList10 = [u'glyburide', u'glyburide',u'im']\n",
    "\n",
    "skipList11 = [u'm',u'doctor',u'started',u'glimeperide']\n",
    "\n",
    "skipList12 = [u'levimir',u'lisinopril',u'metformine']\n",
    "\n",
    "skipList = skipList1 + skipList2 + skipList3 + skipList4\n",
    "\n",
    "skipList = skipList + skipList5 + skipList6 + skipList7 + skipList8 + skipList9 + skipList10 + skipList11 + skipList12\n",
    "\n",
    "#Count number of features in the avoid list\n",
    "print(len(skipList))\n",
    "\n",
    "#count number of features that were there before skip list\n",
    "print(len(refinedfeatures))\n",
    "\n",
    "cleanFeatures = [i for i in refinedfeatures if i not in skipList]\n",
    "\n",
    "#count number of features reamining after filtering out avoid list\n",
    "print(cleanFeatures)\n",
    "\n",
    "#model redefines\n",
    "model = model.filter(features=cleanFeatures)\n",
    "\n",
    "#This will give k fold cross validation results; We can also try SVM, SLP, KNN\n",
    "kfoldcv(KNN, model)\n",
    "\n",
    "#Writing the features and their weights to a csv file\n",
    "listofFeatures = []\n",
    "\n",
    "for i in cleanFeatures:\n",
    "    innerList= [i,model.ig(i)]\n",
    "    listofFeatures.append(innerList)\n",
    "\n",
    "f = open(\"FeaturesKNN.csv\", \"w\")\n",
    "writer = csv1.writer(f,delimiter=',')\n",
    "\n",
    "for item in listofFeatures:\n",
    "    #print(item)\n",
    "    writer.writerow(item)\n",
    "f.close()\n",
    "print(\"Features written to csv file\")"
   ],
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "65\n",
      "46\n",
      "['bathroom', 'cramps', 'diarrhea', 'effects', 'gas', 'nausea', 'pancreatitis', 'stomach', 'tummy', 'upset', 'vomiting', 'allergic', 'nauseated', 'caused', 'pounds', 'nauseous', 'gut', 'weeks', 'hair', 'gi', 'empty', 'effect', 'currently', 'abdominal', 'bowels', 'diarrhoea', 'rash', 'lost', 'awful', 'dose', 'cramping', 'gained']\n",
      "Features written to csv file\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}